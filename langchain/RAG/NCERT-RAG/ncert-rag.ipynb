{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\WellsFargo\\practice\\wf-practive-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\WellsFargo\\practice\\wf-practive-env\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing all docs from NCERT biology books\n",
    "## DEPRECATED\n",
    "all_docs = []\n",
    "for i in range(105, 120):   \n",
    "    print(f\"Loading ncert_book/kebo{i}.pdf\")\n",
    "    loader = PyMuPDFLoader(f\"ncert_book/kebo{i}.pdf\")\n",
    "    docs = loader.load()\n",
    "    all_docs.extend(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìò Extracting ncert_book/kebo105.pdf with PyMuPDF (manual)\n",
      "üìò Extracting ncert_book/kebo106.pdf with PyMuPDF (manual)\n",
      "üìò Extracting ncert_book/kebo107.pdf with PyMuPDF (manual)\n",
      "üìò Extracting ncert_book/kebo108.pdf with PyMuPDF (manual)\n",
      "üìò Extracting ncert_book/kebo109.pdf with PyMuPDF (manual)\n",
      "üìò Extracting ncert_book/kebo110.pdf with PyMuPDF (manual)\n",
      "üìò Extracting ncert_book/kebo111.pdf with PyMuPDF (manual)\n",
      "üìò Extracting ncert_book/kebo112.pdf with PyMuPDF (manual)\n",
      "üìò Extracting ncert_book/kebo113.pdf with PyMuPDF (manual)\n",
      "üìò Extracting ncert_book/kebo114.pdf with PyMuPDF (manual)\n",
      "üìò Extracting ncert_book/kebo115.pdf with PyMuPDF (manual)\n",
      "üìò Extracting ncert_book/kebo116.pdf with PyMuPDF (manual)\n",
      "üìò Extracting ncert_book/kebo117.pdf with PyMuPDF (manual)\n",
      "üìò Extracting ncert_book/kebo118.pdf with PyMuPDF (manual)\n",
      "üìò Extracting ncert_book/kebo119.pdf with PyMuPDF (manual)\n",
      "\n",
      "‚úÖ Total pages extracted: 198\n",
      "üìù Sample text:\n",
      " UNIT 2 The description of the diverse forms of life on earth was made only by observation through naked eyes or later through magnifying lenses and microscopes. This description is mainly of gross structural features, both external and internal. In addition, observable and perceivable living phenomena were also recorded as part of this description. Before experimental biology or more specifically,\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from langchain_core.documents import Document\n",
    "import re\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    # --- Remove headers/footers ---\n",
    "    text = re.sub(r\"Reprint\\s*20\\d{2}-\\d{2}\", \"\", text)\n",
    "    text = re.sub(r\"CHAPTER\\s*\\d+\", \"\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"Page\\s*\\d+\", \"\", text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # --- Remove figure captions and labels ---\n",
    "    text = re.sub(r\"Figure\\s*\\d+(\\.\\d+)*[^\\n]*\", \"\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"Types of [^\\n]*\\n?\", \"\", text)  # e.g. \"Types of aestivation...\"\n",
    "    text = re.sub(r\"\\bdiagram\\b[^\\n]*\", \"\", text, flags=re.IGNORECASE)\n",
    "\n",
    "    # --- Remove loose labels like (a), (b), (c), etc. ---\n",
    "    text = re.sub(r\"\\([a-z]\\)\", \"\", text)\n",
    "    text = re.sub(r\"\\([A-Z]\\)\", \"\", text)\n",
    "\n",
    "    # --- Remove multiple spaces, newlines ---\n",
    "    text = re.sub(r\"\\n+\", \" \", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "    # --- Remove stray punctuation or hyphen artifacts ---\n",
    "    text = text.replace(\"‚Äì\", \"-\").replace(\"‚Äî\", \"-\")\n",
    "    text = re.sub(r\"-\\s+\", \"\", text)  # join hyphenated words split across lines\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "all_docs = []\n",
    "\n",
    "for i in range(105, 120):\n",
    "    pdf_path = f\"ncert_book/kebo{i}.pdf\"\n",
    "    print(f\"üìò Extracting {pdf_path} with PyMuPDF (manual)\")\n",
    "\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        for page in doc:\n",
    "            text = page.get_text(\"text\", flags=1)  # better spacing, Unicode-safe\n",
    "            cleaned = clean_text(text)\n",
    "            if cleaned.strip():\n",
    "                all_docs.append(Document(page_content=cleaned, metadata={\"source\": pdf_path}))\n",
    "\n",
    "print(f\"\\n‚úÖ Total pages extracted: {len(all_docs)}\")\n",
    "print(\"üìù Sample text:\\n\", all_docs[0].page_content[:400])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_docs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **sentence‚Äëtransformers/all‚ÄëMiniLM‚ÄëL6‚Äëv2**\n",
    "\n",
    "   * Lightweight, good efficiency. For many semantic search tasks this is a strong baseline. ([suparva.com][2])\n",
    "   * If your retrieval corpus is moderate size and you don‚Äôt have extremely heavy compute, this is a safe start.\n",
    "\n",
    "2. **sentence‚Äëtransformers/all‚Äëmpnet‚Äëbase‚Äëv2**\n",
    "\n",
    "   * Higher accuracy than MiniLM, somewhat heavier. Many practitioners pick this when quality is more important. (Mentioned in Reddit suggestions). ([Reddit][4])\n",
    "   * Good for textbook/educational content where precision matters.\n",
    "\n",
    "3. **intfloat/e5‚Äëbase** (or variants like e5-small/v2)\n",
    "\n",
    "   * The blog ‚ÄúTop 10 embedding models you should know‚Äù lists E5 as strong for retrieval. ([linkedin.com][5])\n",
    "   * Might be slightly heavier but worth if you want better retrieval.\n",
    "\n",
    "4. **BAAI/bge‚Äëbase‚Äëen‚Äëv1.5**\n",
    "\n",
    "   * Especially recommended in Reddit discussions for retrieval tasks. ([Reddit][6])\n",
    "   * Could be a high‚Äêquality choice if you have the compute and want best performance.\n",
    "\n",
    "5. **Custom / domain fine-tuned embedding model**\n",
    "\n",
    "   * If you find that no out-of-the-box model hits your retrieval accuracy / MCQ context well (because NCERT + NEET style is somewhat niche), you might consider fine-tuning one of the above on your domain (textbook passages + question/answer pairs) so embeddings align well with question retrieval.\n",
    "   * For example you could take mpnet or e5 and fine‚Äêtune via contrastive loss on NCERT passage ‚Üî MCQ answer pairs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "# embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n",
    "\n",
    "llm = ChatGroq(model_name=\"openai/gpt-oss-120B\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"full_ncert.txt\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n----------------------------------------\\n\".join([doc.page_content for doc in all_docs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Creating new vectorstore...\n",
      "‚úÖ Vectorstore saved at: ncert-vector-store\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=450,\n",
    "    chunk_overlap=80,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \"?\", \"!\", \" \"]\n",
    ")\n",
    "chunks = text_splitter.split_documents(all_docs)\n",
    "\n",
    "VECTOR_DB_PATH = \"ncert-vector-store\"\n",
    "\n",
    "if os.path.exists(VECTOR_DB_PATH):\n",
    "    print(\"üîÅ Loading existing vectorstore...\")\n",
    "    vectorstore = FAISS.load_local(VECTOR_DB_PATH, embeddings, allow_dangerous_deserialization=True)\n",
    "    print(\"‚úÖ Existing Vectorstore loaded from:\", VECTOR_DB_PATH)\n",
    "else:\n",
    "    print(\"üß† Creating new vectorstore...\")\n",
    "    vectorstore = FAISS.from_documents(chunks, embedding=embeddings)\n",
    "    # vectorstore.save_local(VECTOR_DB_PATH)\n",
    "    print(\"‚úÖ Vectorstore saved at:\", VECTOR_DB_PATH)\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an NCERT-based NEET Biology assistant.\n",
    "Use only the context provided below to answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Guidelines:\n",
    "- Base your answer strictly on the context (ignore outside knowledge).\n",
    "- If the context does not provide enough info, say:\n",
    "  \"The context does not provide this information.\"\n",
    "- Answer clearly and concisely.\n",
    "If it is an MCQ question it will have 4 options - A, B, C, D \n",
    "Answer just the option - like \"B\"\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm\n",
    "\n",
    "question = \"\"\"\n",
    "Which one of the following is the smallest living cell and lacks a true nucleus?\n",
    "\n",
    "A. Fungi\n",
    "B. Bacterium\n",
    "C. Alga\n",
    "D. Virus\n",
    "\"\"\"\n",
    "\n",
    "def get_answer(question):\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "    context_text = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "    inputs = {\n",
    "        \"context\": context_text,\n",
    "        \"question\": question\n",
    "    }\n",
    "    response = chain.invoke(inputs)\n",
    "\n",
    "    print(\"üß† Question:\", question)\n",
    "    print(\"üí¨ Answer:\", response.content)\n",
    "\n",
    "    output_file = \"retrieved_context_ncert.txt\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n----------------------------------------\\n\".join([doc.page_content for doc in retrieved_docs]))\n",
    "\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Question: The cell organelle responsible for cellular respiration is:\n",
      "A. Ribosome\n",
      "B. Lysosome\n",
      "C. Mitochondrion\n",
      "D. Golgi apparatus\n",
      "üí¨ Answer: C\n",
      "üß† Question: The functional unit of heredity is called:\n",
      "A. Chromosome\n",
      "B. Gene\n",
      "C. DNA\n",
      "D. Codon\n",
      "üí¨ Answer: The context does not provide this information.\n",
      "üß† Question: Which of the following is not a function of the smooth endoplasmic reticulum (SER)?\n",
      "A. Lipid synthesis\n",
      "B. Detoxification\n",
      "C. Protein synthesis\n",
      "D. Steroid hormone production\n",
      "üí¨ Answer: C\n",
      "üß† Question: Which phase of mitosis is characterized by the alignment of chromosomes at the equatorial plate?\n",
      "A. Prophase\n",
      "B. Metaphase\n",
      "C. Anaphase\n",
      "D. Telophase\n",
      "üí¨ Answer: B\n",
      "üß† Question: Which of the following tissues helps in the transportation of food in plants?\n",
      "A. Xylem\n",
      "B. Phloem\n",
      "C. Collenchyma\n",
      "D. Parenchyma\n",
      "üí¨ Answer: B\n",
      "üß† Question: Which among the following is an example of a non-motile, colonial alga?\n",
      "A. Chlamydomonas\n",
      "B. Volvox\n",
      "C. Ulothrix\n",
      "D. Spirogyra\n",
      "üí¨ Answer: The context does not provide this information.\n",
      "üß† Question: The enzyme responsible for the conversion of starch into maltose is:\n",
      "A. Invertase\n",
      "B. Maltase\n",
      "C. Amylase\n",
      "D. Zymase\n",
      "üí¨ Answer: The context does not provide this information.\n",
      "üß† Question: The process of formation of pollen grains from microspore mother cells is known as:\n",
      "A. Sporogenesis\n",
      "B. Microsporogenesis\n",
      "C. Gametogenesis\n",
      "D. Megasporogenesis\n",
      "üí¨ Answer: The context does not provide this information.\n",
      "üß† Question: Which of the following organisms shows a diplontic life cycle?\n",
      "A. Funaria\n",
      "B. Spirogyra\n",
      "C. Pinus\n",
      "D. Polysiphonia\n",
      "üí¨ Answer: The context does not provide this information.\n",
      "üß† Question: The longest phase of the cell cycle is:\n",
      "A. M phase\n",
      "B. G1 phase\n",
      "C. S phase\n",
      "D. G2 phase\n",
      "üí¨ Answer: The context does not provide this information.\n",
      "üß† Question: The apical meristem is responsible for:\n",
      "A. Increase in thickness\n",
      "B. Formation of flowers\n",
      "C. Increase in length\n",
      "D. Leaf fall\n",
      "üí¨ Answer: C\n",
      "üß† Question: The exoskeleton of arthropods is made up of:\n",
      "A. Cellulose\n",
      "B. Chitin\n",
      "C. Keratin\n",
      "D. Collagen\n",
      "üí¨ Answer: B\n",
      "üß† Question: The major nitrogenous waste product in humans is:\n",
      "A. Uric acid\n",
      "B. Ammonia\n",
      "C. Urea\n",
      "D. Creatinine\n",
      "üí¨ Answer: C\n",
      "üß† Question: Which plant hormone is responsible for cell elongation?\n",
      "A. Cytokinin\n",
      "B. Gibberellin\n",
      "C. Auxin\n",
      "D. Abscisic acid\n",
      "üí¨ Answer: B\n",
      "üß† Question: Which one of the following is the smallest living cell and lacks a true nucleus?\n",
      "A. Fungi\n",
      "B. Bacterium\n",
      "C. Alga\n",
      "D. Virus\n",
      "üí¨ Answer: B\n",
      "\n",
      "üìä Evaluation complete! Accuracy: 53.33%\n",
      "\n",
      "‚úÖ Results saved to 'RAG_NCERT_Evaluation_Results.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"NCERT_Biology_Class11_NEET_MCQ.csv\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    full_question = (\n",
    "        f\"{row['Question']}\\n\"\n",
    "        f\"A. {row['Option A']}\\n\"\n",
    "        f\"B. {row['Option B']}\\n\"\n",
    "        f\"C. {row['Option C']}\\n\"\n",
    "        f\"D. {row['Option D']}\"\n",
    "    )\n",
    "\n",
    "    predicted = get_answer(full_question)\n",
    "    predicted_option = predicted.strip().upper()[0] if predicted else \"\"\n",
    "    correct = row[\"Correct Answer\"].strip().upper()\n",
    "    is_correct = predicted_option == correct\n",
    "\n",
    "    results.append({\n",
    "        \"Question\": row[\"Question\"],\n",
    "        \"Predicted\": predicted_option,\n",
    "        \"Correct\": correct,\n",
    "        \"Result\": \"CORRECT\" if is_correct else \"FALSE\"\n",
    "    })\n",
    "\n",
    "eval_df = pd.DataFrame(results)\n",
    "\n",
    "accuracy = (eval_df[\"Result\"] == \"CORRECT\").sum() / len(eval_df) * 100\n",
    "print(f\"\\nüìä Evaluation complete! Accuracy: {accuracy:.2f}%\\n\")\n",
    "\n",
    "# Optionally, save results\n",
    "eval_df.to_csv(\"RAG_NCERT_Evaluation_Results.csv\", index=False)\n",
    "print(\"‚úÖ Results saved to 'RAG_NCERT_Evaluation_Results.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wf-practive-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9dd8f81c8b7834c9610102fe12c8a0a82ea63ba795508c82142f149d0003ca37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
