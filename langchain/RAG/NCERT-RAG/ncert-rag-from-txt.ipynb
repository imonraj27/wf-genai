{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_ollama import ChatOllama\n",
    "import os\n",
    "import pandas as pd\n",
    "from ncert_constants import NCERT_FILE, PAGE_DELIMITER\n",
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(NCERT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "pages = content.split(PAGE_DELIMITER)\n",
    "all_docs = []\n",
    "\n",
    "for i, page in enumerate(pages, start=1):\n",
    "    all_docs.append(Document(page_content=page))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "# embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"intfloat/e5-base\",\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n",
    "# embeddings = HuggingFaceEmbeddings(\n",
    "#     model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "#     encode_kwargs={\"normalize_embeddings\": True}\n",
    "# )\n",
    "\n",
    "# llm = ChatGroq(model_name=\"openai/gpt-oss-20B\", temperature=0)\n",
    "llm = ChatGroq(model_name=\"moonshotai/kimi-k2-instruct-0905\", temperature=0.1)\n",
    "# llm = ChatOllama(model=\"gemma2:2b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=450,\n",
    "    chunk_overlap=80,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \"?\", \"!\", \" \"]\n",
    ")\n",
    "chunks = text_splitter.split_documents(all_docs)\n",
    "\n",
    "VECTOR_DB_PATH = \"ncert-vector-store\"\n",
    "\n",
    "if os.path.exists(VECTOR_DB_PATH):\n",
    "    print(\"üîÅ Loading existing vectorstore...\")\n",
    "    vectorstore = FAISS.load_local(VECTOR_DB_PATH, embeddings, allow_dangerous_deserialization=True)\n",
    "    print(\"‚úÖ Existing Vectorstore loaded from:\", VECTOR_DB_PATH)\n",
    "else:\n",
    "    print(\"üß† Creating new vectorstore...\")\n",
    "    vectorstore = FAISS.from_documents(chunks, embedding=embeddings)\n",
    "    # vectorstore.save_local(VECTOR_DB_PATH)\n",
    "    print(\"‚úÖ Vectorstore saved at:\", VECTOR_DB_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 15})\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an NCERT-based NEET Biology assistant.\n",
    "Use only the context provided below to answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Guidelines:\n",
    "- Base your answer strictly on the context (ignore outside knowledge).\n",
    "- If the context does not provide enough info, say:\n",
    "  \"The context does not provide this information.\"\n",
    "- Answer clearly and concisely.\n",
    "If it is an MCQ question it will have 4 options - A, B, C, D \n",
    "Answer just the option - like \"B\"\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm\n",
    "\n",
    "question = \"\"\"\n",
    "Which one of the following is the smallest living cell and lacks a true nucleus?\n",
    "\n",
    "A. Fungi\n",
    "B. Bacterium\n",
    "C. Alga\n",
    "D. Virus\n",
    "\"\"\"\n",
    "\n",
    "def get_answer(question):\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "    context_text = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "    inputs = {\n",
    "        \"context\": context_text,\n",
    "        \"question\": question\n",
    "    }\n",
    "    response = chain.invoke(inputs)\n",
    "\n",
    "    # print(\"üß† Question:\", question)\n",
    "    # print(\"üí¨ Answer:\", response.content)\n",
    "\n",
    "    output_file = \"retrieved_context_ncert.txt\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n----------------------------------------\\n\".join([doc.page_content for doc in retrieved_docs]))\n",
    "\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- 0 ----\n",
      "CORRECT\n",
      "---- 1 ----\n",
      "CORRECT\n",
      "---- 2 ----\n",
      "CORRECT\n",
      "---- 3 ----\n",
      "CORRECT\n",
      "---- 4 ----\n",
      "CORRECT\n",
      "---- 5 ----\n",
      "CORRECT\n",
      "---- 6 ----\n",
      "CORRECT\n",
      "---- 7 ----\n",
      "CORRECT\n",
      "---- 8 ----\n",
      "CORRECT\n",
      "---- 9 ----\n",
      "CORRECT\n",
      "---- 10 ----\n",
      "WRONG\n",
      "---- 11 ----\n",
      "CORRECT\n",
      "---- 12 ----\n",
      "CORRECT\n",
      "---- 13 ----\n",
      "CORRECT\n",
      "---- 14 ----\n",
      "CORRECT\n",
      "---- 15 ----\n",
      "WRONG\n",
      "---- 16 ----\n",
      "CORRECT\n",
      "---- 17 ----\n",
      "CORRECT\n",
      "---- 18 ----\n",
      "CORRECT\n",
      "---- 19 ----\n",
      "CORRECT\n",
      "---- 20 ----\n",
      "CORRECT\n",
      "---- 21 ----\n",
      "CORRECT\n",
      "---- 22 ----\n",
      "CORRECT\n",
      "---- 23 ----\n",
      "CORRECT\n",
      "---- 24 ----\n",
      "CORRECT\n",
      "---- 25 ----\n",
      "CORRECT\n",
      "---- 26 ----\n",
      "CORRECT\n",
      "---- 27 ----\n",
      "CORRECT\n",
      "---- 28 ----\n",
      "CORRECT\n",
      "---- 29 ----\n",
      "CORRECT\n",
      "---- 30 ----\n",
      "CORRECT\n",
      "---- 31 ----\n",
      "CORRECT\n",
      "---- 32 ----\n",
      "WRONG\n",
      "---- 33 ----\n",
      "CORRECT\n",
      "---- 34 ----\n",
      "CORRECT\n",
      "---- 35 ----\n",
      "CORRECT\n",
      "---- 36 ----\n",
      "CORRECT\n",
      "---- 37 ----\n",
      "CORRECT\n",
      "---- 38 ----\n",
      "CORRECT\n",
      "---- 39 ----\n",
      "CORRECT\n",
      "---- 40 ----\n",
      "CORRECT\n",
      "---- 41 ----\n",
      "CORRECT\n",
      "---- 42 ----\n",
      "CORRECT\n",
      "---- 43 ----\n",
      "CORRECT\n",
      "---- 44 ----\n",
      "CORRECT\n",
      "---- 45 ----\n",
      "CORRECT\n",
      "---- 46 ----\n",
      "CORRECT\n",
      "---- 47 ----\n",
      "WRONG\n",
      "---- 48 ----\n",
      "CORRECT\n",
      "---- 49 ----\n",
      "CORRECT\n",
      "---- 50 ----\n",
      "CORRECT\n",
      "---- 51 ----\n",
      "CORRECT\n",
      "---- 52 ----\n",
      "CORRECT\n",
      "---- 53 ----\n",
      "WRONG\n",
      "---- 54 ----\n",
      "CORRECT\n",
      "---- 55 ----\n",
      "CORRECT\n",
      "---- 56 ----\n",
      "CORRECT\n",
      "---- 57 ----\n",
      "CORRECT\n",
      "---- 58 ----\n",
      "CORRECT\n",
      "---- 59 ----\n",
      "WRONG\n",
      "---- 60 ----\n",
      "WRONG\n",
      "---- 61 ----\n",
      "WRONG\n",
      "---- 62 ----\n",
      "CORRECT\n",
      "---- 63 ----\n",
      "CORRECT\n",
      "---- 64 ----\n",
      "WRONG\n",
      "---- 65 ----\n",
      "CORRECT\n",
      "---- 66 ----\n",
      "CORRECT\n",
      "---- 67 ----\n",
      "CORRECT\n",
      "---- 68 ----\n",
      "CORRECT\n",
      "---- 69 ----\n",
      "CORRECT\n",
      "---- 70 ----\n",
      "CORRECT\n",
      "---- 71 ----\n",
      "WRONG\n",
      "---- 72 ----\n",
      "WRONG\n",
      "---- 73 ----\n",
      "CORRECT\n",
      "---- 74 ----\n",
      "CORRECT\n",
      "---- 75 ----\n",
      "WRONG\n",
      "---- 76 ----\n",
      "CORRECT\n",
      "---- 77 ----\n",
      "CORRECT\n",
      "---- 78 ----\n",
      "CORRECT\n",
      "---- 79 ----\n",
      "CORRECT\n",
      "---- 80 ----\n",
      "CORRECT\n",
      "\n",
      "üìä Evaluation complete! Accuracy: 85.19%\n",
      "\n",
      "‚úÖ Results saved to 'RAG_NCERT_Evaluation_Results.csv'\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"NCERT_Biology_Class11_NEET_MCQ.csv\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    full_question = (\n",
    "        f\"{row['Question']}\\n\"\n",
    "        f\"A. {row['Option A']}\\n\"\n",
    "        f\"B. {row['Option B']}\\n\"\n",
    "        f\"C. {row['Option C']}\\n\"\n",
    "        f\"D. {row['Option D']}\"\n",
    "    )\n",
    "\n",
    "    print(f\"---- {i} ----\")\n",
    "    predicted = get_answer(full_question)\n",
    "    predicted = re.sub(r\"<think>.*?</think>\", \"\", predicted, flags=re.DOTALL).strip()\n",
    "    predicted_option = predicted.strip().upper()[0] if predicted else \"\"\n",
    "    correct = row[\"Correct Answer\"].strip().upper()\n",
    "    is_correct = predicted_option == correct\n",
    "    print(\"CORRECT\" if is_correct else \"WRONG\")\n",
    "\n",
    "    results.append({\n",
    "        \"Question\": row[\"Question\"],\n",
    "        \"Predicted\": predicted_option,\n",
    "        \"Correct\": correct,\n",
    "        \"Result\": \"CORRECT\" if is_correct else \"FALSE\"\n",
    "    })\n",
    "\n",
    "eval_df = pd.DataFrame(results)\n",
    "\n",
    "accuracy = (eval_df[\"Result\"] == \"CORRECT\").sum() / len(eval_df) * 100\n",
    "print(f\"\\nüìä Evaluation complete! Accuracy: {accuracy:.2f}%\\n\")\n",
    "\n",
    "# Optionally, save results\n",
    "eval_df.to_csv(\"RAG_NCERT_Evaluation_Results.csv\", index=False)\n",
    "print(\"‚úÖ Results saved to 'RAG_NCERT_Evaluation_Results.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wf-practive-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9dd8f81c8b7834c9610102fe12c8a0a82ea63ba795508c82142f149d0003ca37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
