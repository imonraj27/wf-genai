{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyMuPDFLoader(\"SherlockHolmesComplete.pdf\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "#sentence-transformers/all-MiniLM-L6-v2\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"intfloat/e5-base-v2\")\n",
    "\n",
    "VECTOR_DB_PATH = \"sherlock-holmes-vectorstore\"\n",
    "\n",
    "if os.path.exists(VECTOR_DB_PATH):\n",
    "    print(\"üîÅ Loading existing vectorstore...\")\n",
    "    vectorstore = FAISS.load_local(VECTOR_DB_PATH, embeddings, allow_dangerous_deserialization=True)\n",
    "    print(\"‚úÖ Existing Vectorstore loaded from:\", VECTOR_DB_PATH)\n",
    "else:\n",
    "    print(\"üß† Creating new vectorstore...\")\n",
    "    vectorstore = FAISS.from_documents(chunks, embedding=embeddings)\n",
    "    vectorstore.save_local(VECTOR_DB_PATH)\n",
    "    print(\"‚úÖ Vectorstore saved at:\", VECTOR_DB_PATH)\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.2)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a helpful assistant. Use the provided context to answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer clearly and concisely. Do not answer anything beyond the context you are given.\n",
    "Even if you have an answer from your existing knowledge you have to answer solely based on the \n",
    "context.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm\n",
    "\n",
    "question = \"Tell me about the 6 Napoleon\"\n",
    "retrieved_docs = retriever.invoke(question)\n",
    "context_text = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "inputs = {\n",
    "    \"context\": context_text,\n",
    "    \"question\": question\n",
    "}\n",
    "response = chain.invoke(inputs)\n",
    "\n",
    "print(\"üß† Question:\", question)\n",
    "print(\"üí¨ Answer:\", response.content)\n",
    "\n",
    "print(f\"\\nüìö Retrieved Context: {context_text}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wf-practive-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9dd8f81c8b7834c9610102fe12c8a0a82ea63ba795508c82142f149d0003ca37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
